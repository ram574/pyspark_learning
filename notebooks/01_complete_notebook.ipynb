{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f451d123-4389-42cd-8e89-1ee7dc8f5841",
   "metadata": {},
   "source": [
    "# <font color=red> PySPark Introduction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0c936-8ae9-4d76-bf10-32e5e01d5af6",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  what is pyspark? \n",
    "\n",
    "- PySpark is a Spark library written in Python to run Python applications using Apache Spark capabilities, using PySpark   we can run applications parallelly on the distributed cluster (multiple nodes).\n",
    "\n",
    "-  PySpark is a Python API for Apache Spark. Apache Spark is an analytical processing engine for large scale powerful     distributed data processing and machine learning applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff6204-5c59-4319-9c81-fade804edad9",
   "metadata": {},
   "source": [
    "# <font color=red>PySpark Features </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883937f-6178-4d79-9a1e-925cdab57ef2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What are Pyspark features\n",
    "\n",
    "- In-memory computation\n",
    "- Distributed processing using parallelize\n",
    "- Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)\n",
    "- Fault-tolerant\n",
    "- Immutable\n",
    "- Lazy evaluation\n",
    "- Cache & persistence\n",
    "- Inbuild-optimization when using DataFrames\n",
    "- Supports ANSI SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555f40e-706c-475e-ba5c-d4678e88d75a",
   "metadata": {},
   "source": [
    "# <font color=red>PySpark Architecture </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aadc49",
   "metadata": {},
   "source": [
    "<img src = \"../database/01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69672c98-e4b3-4b76-8722-411f1d1c4cd0",
   "metadata": {},
   "source": [
    "# <font color=red> Cluster Managers </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759f7b8-03e9-47c8-aad6-58f2ae5a5c32",
   "metadata": {},
   "source": [
    "#### What are the different clusters that can be integrated into pyspark ?\n",
    "\n",
    "- Standalone Cluster: <font color=blue> This is a local cluster and need to setup manually.</font>\n",
    "- Apache Mesos: <font color=blue>  Mesons is a Cluster manager that can also run Hadoop MapReduce and PySpark                                              applications</font>\n",
    "- Hadoop YARN: <font color=blue> This is mostly used, cluster manager which integrated in a hadoop eco system</font>\n",
    "- Kubernets: <font color=blue> An open-source system for automating deployment, scaling, and management of containerized applications.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97dfbf3-4233-4c92-bea3-6094b0dc8a5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <font color=red> PySpark Modules </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bac028-1a61-460d-b2b2-e6d47a7715dd",
   "metadata": {},
   "source": [
    "- PySpark.RDD\n",
    "- Pyspark.sql\n",
    "- Pyspark.streaming\n",
    "- Pyspark.mlib\n",
    "- Pyspark.ml\n",
    "- Pyspark.GraphFrames\n",
    "- Pyspark.resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f44fe-3b6f-4efa-8663-33e9f8d8b7b9",
   "metadata": {},
   "source": [
    "# <font color=red> PySpark Installation </font>\n",
    "\n",
    "- Follow the below link to install PySpark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd5d97-c9e4-4ad8-9c5a-e72f172f943e",
   "metadata": {},
   "source": [
    "# <font color=red>1. Spark Session</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2523c52-fc2e-4621-8bdf-69e23a0bdc31",
   "metadata": {},
   "source": [
    "#### Sparksession has Two Modes\n",
    "  - Client mode\n",
    "  - Cluster mode\n",
    "\n",
    "#### Session includes all the APIs available in different contexts –\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- StreamingContext\n",
    "- HiveContext.\n",
    "\n",
    "<font color=red> Note:</font>\n",
    "- Before spark-2.0 version there is a spark context\n",
    "- From spark-2.0 Sparksession is introduced and developed\n",
    "- Above all contexts are integrated in the sparksession\n",
    "- It is not recommended to create the spark context explicitly becuase spark session itself already have all the above contexts\n",
    "- Remember, stop the spark context always afer using it because you can create only one spark context for one JVM. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393d6fd1-8137-4d5b-be5d-77e27a136d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLient Mode\n",
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('My Application') \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f03c06-ca94-4abf-8595-9a39425c3691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function SparkSession.newSession at 0x000002787FC41A80>\n"
     ]
    }
   ],
   "source": [
    "# Create new SparkSession using newSession method\n",
    "spark2 = SparkSession.newSession\n",
    "print(spark2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5effcc07-48f2-4cfd-816a-aa83855ac27f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x000002787FDC7250>>\n"
     ]
    }
   ],
   "source": [
    "# Get Existing SparkSession\n",
    "spark3 = SparkSession.builder.getOrCreate\n",
    "print(spark3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86154b1-abfd-47a3-b867-6349da0a34b5",
   "metadata": {},
   "source": [
    "# <font color=red> Core Spark Session </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b775f-27bc-4d96-bd84-002eb3e2d8ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color=red >pyspark.sql.SparkSession </font>\n",
    "\n",
    "- The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "- A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over \n",
    "  tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49c1f9-16b0-492f-99e6-336da4d762ef",
   "metadata": {},
   "source": [
    "<font color=red>SparkSession(sparkContext[, jsparkSession, …])</font>\n",
    "\n",
    "- spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Word Count\")\n",
    "        .config(\"spark.some.config.option\", \"some-value\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "- spark = (\n",
    "    SparkSession.builder\n",
    "        .remote(\"sc://localhost\")\n",
    "        .getActive\n",
    "        .config(\"spark.some.config.option\", \"some-value\")\n",
    "        .getOrCreate()\n",
    "    )  ''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd549c6",
   "metadata": {},
   "source": [
    "<img src= \"../database/s1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd380c",
   "metadata": {},
   "source": [
    "<img src = \"../database/s2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0ed93-1ddd-490c-a131-8f19b42ffd07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color=red> pyspark.sql.sparkSession.builder.appName(name) </font>\n",
    "\n",
    "Sets a name for the application, which will be shown in the Spark web UI\n",
    "\n",
    "- syntax: SparkSession.builder.appName(\"My app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf46d04-2987-4232-b55c-5efaec1f0827",
   "metadata": {},
   "source": [
    "### <font color=red>pyspark.sql.SparkSession.builder.config</font>\n",
    "\n",
    "Sets a config option. Options set using this method are automatically propagated to both SparkConf and SparkSession’s own configuration.\n",
    "\n",
    "- from pyspark.conf import SparkConf\n",
    "- SparkSession.builder.config(conf=SparkConf())\n",
    "- SparkSession.builder.config(\"spark.some.config.option\", \"some-value\")\n",
    "- syntax: SparkSession.builder.config(map={\"spark.some.config.number\": 123, \"spark.some.config.float\":              0.123})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca502761-2531-41fc-bdc4-e392fa97f929",
   "metadata": {},
   "source": [
    "### <font color=red> pyspark.sql.SparkSession.builder.enableHiveSupport </font>\n",
    "\n",
    "Enables Hive support, including connectivity to a persistent Hive metastore, support for Hive SerDes, and Hive user-defined functions.\n",
    "\n",
    "- syntax: SparkSession.builder.enableHiveSupport()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61170432-36d9-4f6a-85c3-b48e0bc443ef",
   "metadata": {},
   "source": [
    "### <font color=red> pyspark.sql.SparkSession.builder.getOrCreate </font>\n",
    "\n",
    "- Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder.\n",
    "\n",
    "This method first checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default.\n",
    "\n",
    "- syntax: s1 = SparkSession.builder.config(\"k1\", \"v1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e12f7-d17a-4d83-b07e-5416ea0de890",
   "metadata": {},
   "source": [
    "The configuration of the SparkSession can be changed afterwards\n",
    "\n",
    "- s1.conf.set(\"k1\", \"v1_new\")\n",
    "- s1.conf.get(\"k1\") == \"v1_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda99de-3582-4779-8ed4-e54eb1fa6bdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "In case an existing SparkSession is returned, the config options specified in this builder will be applied to the existing SparkSession.\n",
    "- s2 = SparkSession.builder.config(\"k2\", \"v2\").getOrCreate()\n",
    "- s1.conf.get(\"k1\") == s2.conf.get(\"k1\") == \"v1_new\"\n",
    "- s1.conf.get(\"k2\") == s2.conf.get(\"k2\") == \"v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e94c7b-7f58-4a2f-976e-17365160b2b1",
   "metadata": {},
   "source": [
    "### <font color=red>pyspark.sql.SparkSession.builder.master</font>\n",
    " Sets the Spark master URL to connect to, such as “local” to run locally, “local[4]” to run locally with 4 cores, or “spark://master:7077” to run on a Spark standalone cluster.\n",
    " \n",
    " - syntax: SparkSession.builder.master(\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6624c-8608-4816-98e9-bc10ecb573ac",
   "metadata": {},
   "source": [
    "# <font color=red>2. Spark Context</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d2359-6ad0-4029-96bc-d2803643f757",
   "metadata": {},
   "source": [
    "In PySpark, initializing a SparkContext can be done in a few different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5693b",
   "metadata": {},
   "source": [
    "<img src=\"../database/pys.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717fe4e-326b-4d80-8d5e-c8ccac13978a",
   "metadata": {},
   "source": [
    "<font color=red>1. Using SparkSession (Recommended):</font>\n",
    "\n",
    "- Starting from Spark 2.0, the recommended way to initialize Spark is by using the SparkSession,\n",
    "  which encapsulates both the SparkContext and SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b816b68-c80d-4636-b9cb-4d797146297e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Use spark to create RDDs, DataFrames, etc.\n",
    "\n",
    "# Stop the SparkSession when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89f8b6-096c-4e94-80d5-9a39cd4e1bf1",
   "metadata": {},
   "source": [
    "<font color=red>2. Creating SparkContext Directly (Legacy):</font>\n",
    "\n",
    "- If you're working with an older version of Spark or have specific requirements, \n",
    "  you can create a SparkContext directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe70be9f-8534-40b9-b330-63c871b05176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"My Application\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Use sc to create RDDs and perform operations\n",
    "\n",
    "# Stop the SparkContext when done\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ebea8-b2a4-4c20-bbe5-881e6f26d700",
   "metadata": {},
   "source": [
    "<font color=red>3. Creating SparkContext with Additional Configuration:</font>\n",
    "\n",
    "- If you need to set additional configuration options for your SparkContext, you can do so by passing them in the SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbad94ed-2dcd-4aab-aa6a-0f7d8249a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"My Application\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.some.config.option\", \"config-value\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Use sc to create RDDs and perform operations\n",
    "\n",
    "# Stop the SparkContext when done\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0807920-61f7-4945-b32c-683111f2f4ca",
   "metadata": {},
   "source": [
    "<font color=red>Note</font>\n",
    "\n",
    "-  you can create only one SparkContext per JVM, in order to create another first you need to stop the existing one        using stop() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36843ea7-21c3-481e-bd67-564973f6e612",
   "metadata": {},
   "source": [
    "# <font color=red> SparkContext Commonly Used Methods </font>\n",
    "\n",
    "- accumulator(value[, accum_param]) – It creates an pyspark accumulator variable with initial specified value. Only a     driver can access accumulator variables.\n",
    "\n",
    "- broadcast(value) – read-only PySpark broadcast variable. This will be broadcast to the entire cluster. You can        broadcast a variable to a PySpark cluster only once.\n",
    "\n",
    "- emptyRDD() – Creates an empty RDD\n",
    "\n",
    "- getOrCreate() – Creates or returns a SparkContext\n",
    "\n",
    "- hadoopFile() – Returns an RDD of a Hadoop file\n",
    "\n",
    "- newAPIHadoopFile() – Creates an RDD for a Hadoop file with a new API InputFormat.\n",
    "\n",
    "- sequenceFile() – Get an RDD for a Hadoop SequenceFile with given key and value types.\n",
    "\n",
    "- setLogLevel() – Change log level to debug, info, warn, fatal, and error\n",
    "\n",
    "- textFile() – Reads a text file from HDFS, local or any Hadoop supported file systems and returns an RDD\n",
    "\n",
    "- union() – Union two RDDs\n",
    "\n",
    "- wholeTextFiles() – Reads a text file in the folder from HDFS, local or any Hadoop supported file systems and returns an RDD of Tuple2. The first element of the tuple consists file name and the second element consists context of the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b0cc2-4c27-4754-96d8-ef338c3c309d",
   "metadata": {},
   "source": [
    "# <font color=red> 3.RDD (Resilient Distributed Dataset) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406950f8-70dc-4990-b474-9e514fe16154",
   "metadata": {},
   "source": [
    "# <font color=red>What is a RDD?</font>\n",
    "\n",
    "- RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. \n",
    "\n",
    "- In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3d146",
   "metadata": {},
   "source": [
    "<img src = \"../database/rdd1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b57b98-f08d-44c6-aa80-551813cb132d",
   "metadata": {},
   "source": [
    "# <font color=red>RDD Benefits</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542cedc-9c78-4c26-8fca-31f5a9bb2c5b",
   "metadata": {},
   "source": [
    "- In-Memory Processing\n",
    "<font color=blue> PySpark loads the data from disk and process in memory and keeps the data in memory, this is the main difference between PySpark and Mapreduce (I/O intensive). In between the transformations, we can also cache/persists the RDD in memory to reuse the previous computations.</font>\n",
    "\n",
    "- Immutability\n",
    "<font color=blue>PySpark RDD’s are immutable in nature meaning, once RDDs are created you cannot modify. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.</font>\n",
    "\n",
    "- Fault Tolerance\n",
    "<font color=blue>PySpark operates on fault-tolerant data stores on HDFS, S3 e.t.c hence any RDD operation fails, it automatically reloads the data from other partitions. Also, When PySpark applications running on a cluster, PySpark task failures are automatically recovered for a certain number of times (as per the configuration) and finish the application seamlessly.</font>\n",
    "\n",
    "- Lazy Evolution\n",
    "<font color=blue>PySpark does not evaluate the RDD transformations as they appear/encountered by Driver instead it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action.\n",
    "</font>\n",
    "\n",
    "- Partitioning\n",
    "<font color=blue> When you create RDD from a data, It by default partitions the elements in a RDD. By default it partitions to the number of cores available.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b6ead-efb9-4b98-816b-333fa35ff76e",
   "metadata": {},
   "source": [
    "#### <font color=red> what types of data structures can rdd does process ? </font>\n",
    "\n",
    "- RDD (Resilient Distributed Dataset) in Spark is designed to store and manage any type of data. It is a fundamental abstraction that can hold a wide range of data structures, including:\n",
    "\n",
    "1. **Basic Types**: RDDs can store simple types such as integers, floats, strings, and booleans.\n",
    "\n",
    "2. **Tuples**: RDDs can store tuples, which are ordered collections of elements. Tuples can contain a mix of different data types.\n",
    "\n",
    "3. **Lists and Arrays**: RDDs can store lists or arrays of elements. Each element in the RDD can be a list or array of arbitrary length.\n",
    "\n",
    "4. **Dictionaries/Maps**: RDDs can store dictionaries or maps, which are key-value pairs. Each element in the RDD can be a dictionary with key-value pairs.\n",
    "\n",
    "5. **Custom Objects**: RDDs can store instances of custom classes or objects that you define. However, the objects need to be serializable since RDDs are distributed across the cluster.\n",
    "\n",
    "6. **Structured Data**: RDDs can store structured data, such as rows or records with named fields.\n",
    "\n",
    "7. **Nested Structures**: RDDs can store complex nested structures, where elements can be combinations of lists, tuples, dictionaries, or custom objects.\n",
    "\n",
    "Keep in mind that while RDDs can store a variety of data structures, the actual data type and structure of the RDD is determined by how you create and transform it. The operations you perform on the RDD, such as `map`, `filter`, and `reduce`, influence the structure and type of the resulting RDD.\n",
    "\n",
    "It's also important to note that while RDDs offer a lot of flexibility in terms of data types, they don't provide built-in optimizations for complex data types as compared to higher-level abstractions like DataFrames or Datasets in Spark. These higher-level abstractions provide schema information and optimizations that make working with structured and semi-structured data more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44aadc-eddd-422b-aaf2-587441240a31",
   "metadata": {},
   "source": [
    "# <font color=red>Creating RDD's</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71225fa-09dd-47a1-b8db-2ccd7d32024b",
   "metadata": {},
   "source": [
    "- RDD’s are created primarily in two different ways\n",
    "    - parallelizing collection \n",
    "      (creating a new collection from an existing collection)\n",
    "    - referencing a dataset from an external storage system (HDFS, S3 and many more). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55755599",
   "metadata": {},
   "source": [
    "<img src = \"../database/rdd2.jpg\" width = 700 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98516908-4701-4366-b013-7e832299b852",
   "metadata": {},
   "source": [
    "## <font color=red>First Method: Parallelized Collections</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad05e1d-797e-466a-92e3-2d684f8ca085",
   "metadata": {},
   "source": [
    "#### <font color=red>Create RDD using sparkContext.parallelize()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94a7728-8a6f-49b8-a6b9-715ea74a717d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "<bound method SparkContext.emptyRDD of <SparkContext master=local[*] appName=My Application>>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Firstly, you need to create a spark session which contains sparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"My Application\").getOrCreate()    \n",
    "\n",
    "# This is the basic method in creating RDD.\n",
    "# When you alreaady have data in memory that is either loaded from a file or a database.\n",
    "# Initialising RDD usoing Parallelize()\n",
    "\n",
    "data = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "print(rdd.collect())\n",
    "\n",
    "### Creating an empy RDD \n",
    "\n",
    "rdd1 = spark.sparkContext.emptyRDD\n",
    "\n",
    "print(rdd1)\n",
    "\n",
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions\n",
    "\n",
    "print(rdd2.collect())\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afd82b-5f7a-48ed-9dee-84c4036349ff",
   "metadata": {},
   "source": [
    "## <font color=red>Second Method: External Datasets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ff3a66-0eea-4122-9a2c-e72dec9389c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Car_Name', 'Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n",
      "       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner'],\n",
      "      dtype='object')\n",
      "[[3.35, 'ritz'], [4.75, 'sx4'], [7.25, 'ciaz'], [2.85, 'wagon r'], [4.6, 'swift'], [9.25, 'vitara brezza'], [6.75, 'ciaz'], [6.5, 's cross'], [8.75, 'ciaz'], [7.45, 'ciaz'], [2.85, 'alto 800'], [6.85, 'ciaz'], [7.5, 'ciaz'], [6.1, 'ertiga'], [2.25, 'dzire'], [7.75, 'ertiga'], [7.25, 'ertiga'], [7.75, 'ertiga'], [3.25, 'wagon r'], [2.65, 'sx4'], [2.85, 'alto k10'], [4.9, 'ignis'], [4.4, 'sx4'], [2.5, 'alto k10'], [2.9, 'wagon r'], [3.0, 'swift'], [4.15, 'swift'], [6.0, 'swift'], [1.95, 'alto k10'], [7.45, 'ciaz'], [3.1, 'ritz'], [2.35, 'ritz'], [4.95, 'swift'], [6.0, 'ertiga'], [5.5, 'dzire'], [2.95, 'sx4'], [4.65, 'dzire'], [0.35, '800'], [3.0, 'alto k10'], [2.25, 'sx4'], [5.85, 'baleno'], [2.55, 'alto k10'], [1.95, 'sx4'], [5.5, 'dzire'], [1.25, 'omni'], [7.5, 'ciaz'], [2.65, 'ritz'], [1.05, 'wagon r'], [5.8, 'ertiga'], [7.75, 'ciaz'], [14.9, 'fortuner'], [23.0, 'fortuner'], [18.0, 'innova'], [16.0, 'fortuner'], [2.75, 'innova'], [3.6, 'corolla altis'], [4.5, 'etios cross'], [4.75, 'corolla altis'], [4.1, 'etios g'], [19.99, 'fortuner'], [6.95, 'corolla altis'], [4.5, 'etios cross'], [18.75, 'fortuner'], [23.5, 'fortuner'], [33.0, 'fortuner'], [4.75, 'etios liva'], [19.75, 'innova'], [9.25, 'fortuner'], [4.35, 'corolla altis'], [14.25, 'corolla altis'], [3.95, 'etios liva'], [4.5, 'corolla altis'], [7.45, 'corolla altis'], [2.65, 'etios liva'], [4.9, 'etios cross'], [3.95, 'etios g'], [5.5, 'corolla altis'], [1.5, 'corolla'], [5.25, 'corolla altis'], [14.5, 'fortuner'], [14.73, 'corolla altis'], [4.75, 'etios gd'], [23.0, 'innova'], [12.5, 'innova'], [3.49, 'innova'], [2.5, 'camry'], [35.0, 'land cruiser'], [5.9, 'corolla altis'], [3.45, 'etios liva'], [4.75, 'etios g'], [3.8, 'corolla altis'], [11.25, 'innova'], [3.51, 'innova'], [23.0, 'fortuner'], [4.0, 'corolla altis'], [5.85, 'corolla altis'], [20.75, 'innova'], [17.0, 'corolla altis'], [7.05, 'corolla altis'], [9.65, 'fortuner'], [1.75, 'Royal Enfield Thunder 500'], [1.7, 'UM Renegade Mojave'], [1.65, 'KTM RC200'], [1.45, 'Bajaj Dominar 400'], [1.35, 'Royal Enfield Classic 350'], [1.35, 'KTM RC390'], [1.35, 'Hyosung GT250R'], [1.25, 'Royal Enfield Thunder 350'], [1.2, 'Royal Enfield Thunder 350'], [1.2, 'Royal Enfield Classic 350'], [1.2, 'KTM RC200'], [1.15, 'Royal Enfield Thunder 350'], [1.15, 'KTM 390 Duke '], [1.15, 'Mahindra Mojo XT300'], [1.15, 'Royal Enfield Classic 350'], [1.11, 'Royal Enfield Classic 350'], [1.1, 'Royal Enfield Classic 350'], [1.1, 'Royal Enfield Thunder 500'], [1.1, 'Royal Enfield Classic 350'], [1.05, 'Royal Enfield Thunder 500'], [1.05, 'Bajaj Pulsar RS200'], [1.05, 'Royal Enfield Thunder 350'], [1.05, 'Royal Enfield Bullet 350'], [1.0, 'Royal Enfield Classic 350'], [0.95, 'Royal Enfield Classic 500'], [0.9, 'Royal Enfield Classic 500'], [0.9, 'Bajaj Avenger 220'], [0.75, 'Bajaj Avenger 150'], [0.8, 'Honda CB Hornet 160R'], [0.78, 'Yamaha FZ S V 2.0'], [0.75, 'Honda CB Hornet 160R'], [0.75, 'Yamaha FZ 16'], [0.75, 'Bajaj Avenger 220'], [0.72, 'Bajaj Avenger 220'], [0.65, 'TVS Apache RTR 160'], [0.65, 'Bajaj Pulsar 150'], [0.65, 'Honda CBR 150'], [0.65, 'Hero Extreme'], [0.6, 'Honda CB Hornet 160R'], [0.6, 'Bajaj Avenger 220 dtsi'], [0.6, 'Honda CBR 150'], [0.6, 'Bajaj Avenger 150 street'], [0.6, 'Yamaha FZ  v 2.0'], [0.6, 'Yamaha FZ  v 2.0'], [0.6, 'Bajaj Pulsar  NS 200'], [0.6, 'TVS Apache RTR 160'], [0.55, 'Hero Extreme'], [0.55, 'Yamaha FZ S V 2.0'], [0.52, 'Bajaj Pulsar 220 F'], [0.51, 'Bajaj Pulsar 220 F'], [0.5, 'TVS Apache RTR 180'], [0.5, 'Hero Passion X pro'], [0.5, 'Bajaj Pulsar NS 200'], [0.5, 'Bajaj Pulsar NS 200'], [0.5, 'Yamaha Fazer '], [0.48, 'Honda Activa 4G'], [0.48, 'TVS Sport '], [0.48, 'Yamaha FZ S V 2.0'], [0.48, 'Honda Dream Yuga '], [0.45, 'Honda Activa 4G'], [0.45, 'Bajaj Avenger Street 220'], [0.45, 'TVS Apache RTR 180'], [0.45, 'Bajaj Pulsar NS 200'], [0.45, 'Bajaj Avenger 220 dtsi'], [0.45, 'Hero Splender iSmart'], [0.45, 'Activa 3g'], [0.45, 'Hero Passion Pro'], [0.42, 'TVS Apache RTR 160'], [0.42, 'Honda CB Trigger'], [0.4, 'Hero Splender iSmart'], [0.4, 'Yamaha FZ S '], [0.4, 'Hero Passion Pro'], [0.4, 'Bajaj Pulsar 135 LS'], [0.4, 'Activa 4g'], [0.38, 'Honda CB Unicorn'], [0.38, 'Hero Honda CBZ extreme'], [0.35, 'Honda Karizma'], [0.35, 'Honda Activa 125'], [0.35, 'TVS Jupyter'], [0.31, 'Honda Karizma'], [0.3, 'Hero Honda Passion Pro'], [0.3, 'Hero Splender Plus'], [0.3, 'Honda CB Shine'], [0.27, 'Bajaj Discover 100'], [0.25, 'Bajaj Pulsar 150'], [0.25, 'Suzuki Access 125'], [0.25, 'TVS Wego'], [0.25, 'Honda CB twister'], [0.25, 'Hero Glamour'], [0.2, 'Hero Super Splendor'], [0.2, 'Bajaj Pulsar 150'], [0.2, 'Bajaj Discover 125'], [0.2, 'Hero Hunk'], [0.2, 'Hero  Ignitor Disc'], [0.2, 'Hero  CBZ Xtreme'], [0.18, 'Bajaj  ct 100'], [0.17, 'Activa 3g'], [0.16, 'Honda CB twister'], [0.15, 'Bajaj Discover 125'], [0.12, 'Honda CB Shine'], [0.1, 'Bajaj Pulsar 150'], [3.25, 'i20'], [4.4, 'grand i10'], [2.95, 'i10'], [2.75, 'eon'], [5.25, 'grand i10'], [5.75, 'xcent'], [5.15, 'grand i10'], [7.9, 'i20'], [4.85, 'grand i10'], [3.1, 'i10'], [11.75, 'elantra'], [11.25, 'creta'], [2.9, 'i20'], [5.25, 'grand i10'], [4.5, 'verna'], [2.9, 'eon'], [3.15, 'eon'], [6.45, 'verna'], [4.5, 'verna'], [3.5, 'eon'], [4.5, 'i20'], [6.0, 'i20'], [8.25, 'verna'], [5.11, 'verna'], [2.7, 'i10'], [5.25, 'grand i10'], [2.55, 'i10'], [4.95, 'verna'], [3.1, 'i20'], [6.15, 'verna'], [9.25, 'verna'], [11.45, 'elantra'], [3.9, 'grand i10'], [5.5, 'grand i10'], [9.1, 'verna'], [3.1, 'eon'], [11.25, 'creta'], [4.8, 'verna'], [2.0, 'eon'], [5.35, 'verna'], [4.75, 'xcent'], [4.4, 'xcent'], [6.25, 'i20'], [5.95, 'verna'], [5.2, 'verna'], [3.75, 'i20'], [5.95, 'verna'], [4.0, 'i10'], [5.25, 'i20'], [12.9, 'creta'], [5.0, 'city'], [5.4, 'brio'], [7.2, 'city'], [5.25, 'city'], [3.0, 'brio'], [10.25, 'city'], [8.5, 'city'], [8.4, 'city'], [3.9, 'amaze'], [9.15, 'city'], [5.5, 'brio'], [4.0, 'amaze'], [6.6, 'jazz'], [4.0, 'amaze'], [6.5, 'jazz'], [3.65, 'amaze'], [8.35, 'city'], [4.8, 'brio'], [6.7, 'city'], [4.1, 'city'], [3.0, 'city'], [7.5, 'city'], [2.25, 'jazz'], [5.3, 'brio'], [10.9, 'city'], [8.65, 'city'], [9.7, 'city'], [6.0, 'jazz'], [6.25, 'city'], [5.25, 'brio'], [2.1, 'city'], [8.25, 'city'], [8.99, 'city'], [3.5, 'brio'], [7.4, 'jazz'], [5.65, 'jazz'], [5.75, 'amaze'], [8.4, 'city'], [10.11, 'city'], [4.5, 'amaze'], [5.4, 'brio'], [6.4, 'jazz'], [3.25, 'city'], [3.75, 'amaze'], [8.55, 'city'], [9.5, 'city'], [4.0, 'brio'], [3.35, 'city'], [11.5, 'city'], [5.3, 'brio']]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "df = data[[\"Selling_Price\",\"Car_Name\"]]\n",
    "\n",
    "df1 = df.values.tolist()\n",
    "\n",
    "rdd_df = spark.sparkContext.parallelize(df1)\n",
    "\n",
    "print(rdd_df.collect())\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962f30e5-8b94-45e9-8099-0563dcde87cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "|     Car_Name|Year|Selling_Price|Present_Price|Kms_Driven|Fuel_Type|Seller_Type|Transmission|Owner|\n",
      "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "|         ritz|2014|         3.35|         5.59|     27000|   Petrol|     Dealer|      Manual|    0|\n",
      "|          sx4|2013|         4.75|         9.54|     43000|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2017|         7.25|         9.85|      6900|   Petrol|     Dealer|      Manual|    0|\n",
      "|      wagon r|2011|         2.85|         4.15|      5200|   Petrol|     Dealer|      Manual|    0|\n",
      "|        swift|2014|          4.6|         6.87|     42450|   Diesel|     Dealer|      Manual|    0|\n",
      "|vitara brezza|2018|         9.25|         9.83|      2071|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|         6.75|         8.12|     18796|   Petrol|     Dealer|      Manual|    0|\n",
      "|      s cross|2015|          6.5|         8.61|     33429|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2016|         8.75|         8.89|     20273|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|         7.45|         8.92|     42367|   Diesel|     Dealer|      Manual|    0|\n",
      "|     alto 800|2017|         2.85|          3.6|      2135|   Petrol|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|         6.85|        10.38|     51000|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|          7.5|         9.94|     15000|   Petrol|     Dealer|   Automatic|    0|\n",
      "|       ertiga|2015|          6.1|         7.71|     26000|   Petrol|     Dealer|      Manual|    0|\n",
      "|        dzire|2009|         2.25|         7.21|     77427|   Petrol|     Dealer|      Manual|    0|\n",
      "|       ertiga|2016|         7.75|        10.79|     43000|   Diesel|     Dealer|      Manual|    0|\n",
      "|       ertiga|2015|         7.25|        10.79|     41678|   Diesel|     Dealer|      Manual|    0|\n",
      "|       ertiga|2016|         7.75|        10.79|     43000|   Diesel|     Dealer|      Manual|    0|\n",
      "|      wagon r|2015|         3.25|         5.09|     35500|      CNG|     Dealer|      Manual|    0|\n",
      "|          sx4|2010|         2.65|         7.98|     41442|   Petrol|     Dealer|      Manual|    0|\n",
      "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "data = spark.read.csv(\"../input/car_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "data.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bb6de-5bf3-4da3-ba93-74b9f2ff5d64",
   "metadata": {},
   "source": [
    "### <font color=red> Conversions of data columns from one structure to another </font >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e79dc2-10d3-475f-87a8-d6f00a7cf18b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list elements: [['ritz', 3.35], ['sx4', 4.75], ['ciaz', 7.25], ['wagon r', 2.85], ['swift', 4.6], ['vitara brezza', 9.25], ['ciaz', 6.75], ['s cross', 6.5], ['ciaz', 8.75], ['ciaz', 7.45], ['alto 800', 2.85], ['ciaz', 6.85], ['ciaz', 7.5], ['ertiga', 6.1], ['dzire', 2.25], ['ertiga', 7.75], ['ertiga', 7.25], ['ertiga', 7.75], ['wagon r', 3.25], ['sx4', 2.65], ['alto k10', 2.85], ['ignis', 4.9], ['sx4', 4.4], ['alto k10', 2.5], ['wagon r', 2.9], ['swift', 3.0], ['swift', 4.15], ['swift', 6.0], ['alto k10', 1.95], ['ciaz', 7.45], ['ritz', 3.1], ['ritz', 2.35], ['swift', 4.95], ['ertiga', 6.0], ['dzire', 5.5], ['sx4', 2.95], ['dzire', 4.65], ['800', 0.35], ['alto k10', 3.0], ['sx4', 2.25], ['baleno', 5.85], ['alto k10', 2.55], ['sx4', 1.95], ['dzire', 5.5], ['omni', 1.25], ['ciaz', 7.5], ['ritz', 2.65], ['wagon r', 1.05], ['ertiga', 5.8], ['ciaz', 7.75], ['fortuner', 14.9], ['fortuner', 23.0], ['innova', 18.0], ['fortuner', 16.0], ['innova', 2.75], ['corolla altis', 3.6], ['etios cross', 4.5], ['corolla altis', 4.75], ['etios g', 4.1], ['fortuner', 19.99], ['corolla altis', 6.95], ['etios cross', 4.5], ['fortuner', 18.75], ['fortuner', 23.5], ['fortuner', 33.0], ['etios liva', 4.75], ['innova', 19.75], ['fortuner', 9.25], ['corolla altis', 4.35], ['corolla altis', 14.25], ['etios liva', 3.95], ['corolla altis', 4.5], ['corolla altis', 7.45], ['etios liva', 2.65], ['etios cross', 4.9], ['etios g', 3.95], ['corolla altis', 5.5], ['corolla', 1.5], ['corolla altis', 5.25], ['fortuner', 14.5], ['corolla altis', 14.73], ['etios gd', 4.75], ['innova', 23.0], ['innova', 12.5], ['innova', 3.49], ['camry', 2.5], ['land cruiser', 35.0], ['corolla altis', 5.9], ['etios liva', 3.45], ['etios g', 4.75], ['corolla altis', 3.8], ['innova', 11.25], ['innova', 3.51], ['fortuner', 23.0], ['corolla altis', 4.0], ['corolla altis', 5.85], ['innova', 20.75], ['corolla altis', 17.0], ['corolla altis', 7.05], ['fortuner', 9.65], ['Royal Enfield Thunder 500', 1.75], ['UM Renegade Mojave', 1.7], ['KTM RC200', 1.65], ['Bajaj Dominar 400', 1.45], ['Royal Enfield Classic 350', 1.35], ['KTM RC390', 1.35], ['Hyosung GT250R', 1.35], ['Royal Enfield Thunder 350', 1.25], ['Royal Enfield Thunder 350', 1.2], ['Royal Enfield Classic 350', 1.2], ['KTM RC200', 1.2], ['Royal Enfield Thunder 350', 1.15], ['KTM 390 Duke ', 1.15], ['Mahindra Mojo XT300', 1.15], ['Royal Enfield Classic 350', 1.15], ['Royal Enfield Classic 350', 1.11], ['Royal Enfield Classic 350', 1.1], ['Royal Enfield Thunder 500', 1.1], ['Royal Enfield Classic 350', 1.1], ['Royal Enfield Thunder 500', 1.05], ['Bajaj Pulsar RS200', 1.05], ['Royal Enfield Thunder 350', 1.05], ['Royal Enfield Bullet 350', 1.05], ['Royal Enfield Classic 350', 1.0], ['Royal Enfield Classic 500', 0.95], ['Royal Enfield Classic 500', 0.9], ['Bajaj Avenger 220', 0.9], ['Bajaj Avenger 150', 0.75], ['Honda CB Hornet 160R', 0.8], ['Yamaha FZ S V 2.0', 0.78], ['Honda CB Hornet 160R', 0.75], ['Yamaha FZ 16', 0.75], ['Bajaj Avenger 220', 0.75], ['Bajaj Avenger 220', 0.72], ['TVS Apache RTR 160', 0.65], ['Bajaj Pulsar 150', 0.65], ['Honda CBR 150', 0.65], ['Hero Extreme', 0.65], ['Honda CB Hornet 160R', 0.6], ['Bajaj Avenger 220 dtsi', 0.6], ['Honda CBR 150', 0.6], ['Bajaj Avenger 150 street', 0.6], ['Yamaha FZ  v 2.0', 0.6], ['Yamaha FZ  v 2.0', 0.6], ['Bajaj Pulsar  NS 200', 0.6], ['TVS Apache RTR 160', 0.6], ['Hero Extreme', 0.55], ['Yamaha FZ S V 2.0', 0.55], ['Bajaj Pulsar 220 F', 0.52], ['Bajaj Pulsar 220 F', 0.51], ['TVS Apache RTR 180', 0.5], ['Hero Passion X pro', 0.5], ['Bajaj Pulsar NS 200', 0.5], ['Bajaj Pulsar NS 200', 0.5], ['Yamaha Fazer ', 0.5], ['Honda Activa 4G', 0.48], ['TVS Sport ', 0.48], ['Yamaha FZ S V 2.0', 0.48], ['Honda Dream Yuga ', 0.48], ['Honda Activa 4G', 0.45], ['Bajaj Avenger Street 220', 0.45], ['TVS Apache RTR 180', 0.45], ['Bajaj Pulsar NS 200', 0.45], ['Bajaj Avenger 220 dtsi', 0.45], ['Hero Splender iSmart', 0.45], ['Activa 3g', 0.45], ['Hero Passion Pro', 0.45], ['TVS Apache RTR 160', 0.42], ['Honda CB Trigger', 0.42], ['Hero Splender iSmart', 0.4], ['Yamaha FZ S ', 0.4], ['Hero Passion Pro', 0.4], ['Bajaj Pulsar 135 LS', 0.4], ['Activa 4g', 0.4], ['Honda CB Unicorn', 0.38], ['Hero Honda CBZ extreme', 0.38], ['Honda Karizma', 0.35], ['Honda Activa 125', 0.35], ['TVS Jupyter', 0.35], ['Honda Karizma', 0.31], ['Hero Honda Passion Pro', 0.3], ['Hero Splender Plus', 0.3], ['Honda CB Shine', 0.3], ['Bajaj Discover 100', 0.27], ['Bajaj Pulsar 150', 0.25], ['Suzuki Access 125', 0.25], ['TVS Wego', 0.25], ['Honda CB twister', 0.25], ['Hero Glamour', 0.25], ['Hero Super Splendor', 0.2], ['Bajaj Pulsar 150', 0.2], ['Bajaj Discover 125', 0.2], ['Hero Hunk', 0.2], ['Hero  Ignitor Disc', 0.2], ['Hero  CBZ Xtreme', 0.2], ['Bajaj  ct 100', 0.18], ['Activa 3g', 0.17], ['Honda CB twister', 0.16], ['Bajaj Discover 125', 0.15], ['Honda CB Shine', 0.12], ['Bajaj Pulsar 150', 0.1], ['i20', 3.25], ['grand i10', 4.4], ['i10', 2.95], ['eon', 2.75], ['grand i10', 5.25], ['xcent', 5.75], ['grand i10', 5.15], ['i20', 7.9], ['grand i10', 4.85], ['i10', 3.1], ['elantra', 11.75], ['creta', 11.25], ['i20', 2.9], ['grand i10', 5.25], ['verna', 4.5], ['eon', 2.9], ['eon', 3.15], ['verna', 6.45], ['verna', 4.5], ['eon', 3.5], ['i20', 4.5], ['i20', 6.0], ['verna', 8.25], ['verna', 5.11], ['i10', 2.7], ['grand i10', 5.25], ['i10', 2.55], ['verna', 4.95], ['i20', 3.1], ['verna', 6.15], ['verna', 9.25], ['elantra', 11.45], ['grand i10', 3.9], ['grand i10', 5.5], ['verna', 9.1], ['eon', 3.1], ['creta', 11.25], ['verna', 4.8], ['eon', 2.0], ['verna', 5.35], ['xcent', 4.75], ['xcent', 4.4], ['i20', 6.25], ['verna', 5.95], ['verna', 5.2], ['i20', 3.75], ['verna', 5.95], ['i10', 4.0], ['i20', 5.25], ['creta', 12.9], ['city', 5.0], ['brio', 5.4], ['city', 7.2], ['city', 5.25], ['brio', 3.0], ['city', 10.25], ['city', 8.5], ['city', 8.4], ['amaze', 3.9], ['city', 9.15], ['brio', 5.5], ['amaze', 4.0], ['jazz', 6.6], ['amaze', 4.0], ['jazz', 6.5], ['amaze', 3.65], ['city', 8.35], ['brio', 4.8], ['city', 6.7], ['city', 4.1], ['city', 3.0], ['city', 7.5], ['jazz', 2.25], ['brio', 5.3], ['city', 10.9], ['city', 8.65], ['city', 9.7], ['jazz', 6.0], ['city', 6.25], ['brio', 5.25], ['city', 2.1], ['city', 8.25], ['city', 8.99], ['brio', 3.5], ['jazz', 7.4], ['jazz', 5.65], ['amaze', 5.75], ['city', 8.4], ['city', 10.11], ['amaze', 4.5], ['brio', 5.4], ['jazz', 6.4], ['city', 3.25], ['amaze', 3.75], ['city', 8.55], ['city', 9.5], ['brio', 4.0], ['city', 3.35], ['city', 11.5], ['brio', 5.3]]\n",
      "dictionaries: {'ritz': 2.65, 'sx4': 1.95, 'ciaz': 7.75, 'wagon r': 1.05, 'swift': 4.95, 'vitara brezza': 9.25, 's cross': 6.5, 'alto 800': 2.85, 'ertiga': 5.8, 'dzire': 5.5, 'alto k10': 2.55, 'ignis': 4.9, '800': 0.35, 'baleno': 5.85, 'omni': 1.25, 'fortuner': 9.65, 'innova': 20.75, 'corolla altis': 7.05, 'etios cross': 4.9, 'etios g': 4.75, 'etios liva': 3.45, 'corolla': 1.5, 'etios gd': 4.75, 'camry': 2.5, 'land cruiser': 35.0, 'Royal Enfield Thunder 500': 1.05, 'UM Renegade Mojave': 1.7, 'KTM RC200': 1.2, 'Bajaj Dominar 400': 1.45, 'Royal Enfield Classic 350': 1.0, 'KTM RC390': 1.35, 'Hyosung GT250R': 1.35, 'Royal Enfield Thunder 350': 1.05, 'KTM 390 Duke ': 1.15, 'Mahindra Mojo XT300': 1.15, 'Bajaj Pulsar RS200': 1.05, 'Royal Enfield Bullet 350': 1.05, 'Royal Enfield Classic 500': 0.9, 'Bajaj Avenger 220': 0.72, 'Bajaj Avenger 150': 0.75, 'Honda CB Hornet 160R': 0.6, 'Yamaha FZ S V 2.0': 0.48, 'Yamaha FZ 16': 0.75, 'TVS Apache RTR 160': 0.42, 'Bajaj Pulsar 150': 0.1, 'Honda CBR 150': 0.6, 'Hero Extreme': 0.55, 'Bajaj Avenger 220 dtsi': 0.45, 'Bajaj Avenger 150 street': 0.6, 'Yamaha FZ  v 2.0': 0.6, 'Bajaj Pulsar  NS 200': 0.6, 'Bajaj Pulsar 220 F': 0.51, 'TVS Apache RTR 180': 0.45, 'Hero Passion X pro': 0.5, 'Bajaj Pulsar NS 200': 0.45, 'Yamaha Fazer ': 0.5, 'Honda Activa 4G': 0.45, 'TVS Sport ': 0.48, 'Honda Dream Yuga ': 0.48, 'Bajaj Avenger Street 220': 0.45, 'Hero Splender iSmart': 0.4, 'Activa 3g': 0.17, 'Hero Passion Pro': 0.4, 'Honda CB Trigger': 0.42, 'Yamaha FZ S ': 0.4, 'Bajaj Pulsar 135 LS': 0.4, 'Activa 4g': 0.4, 'Honda CB Unicorn': 0.38, 'Hero Honda CBZ extreme': 0.38, 'Honda Karizma': 0.31, 'Honda Activa 125': 0.35, 'TVS Jupyter': 0.35, 'Hero Honda Passion Pro': 0.3, 'Hero Splender Plus': 0.3, 'Honda CB Shine': 0.12, 'Bajaj Discover 100': 0.27, 'Suzuki Access 125': 0.25, 'TVS Wego': 0.25, 'Honda CB twister': 0.16, 'Hero Glamour': 0.25, 'Hero Super Splendor': 0.2, 'Bajaj Discover 125': 0.15, 'Hero Hunk': 0.2, 'Hero  Ignitor Disc': 0.2, 'Hero  CBZ Xtreme': 0.2, 'Bajaj  ct 100': 0.18, 'i20': 5.25, 'grand i10': 5.5, 'i10': 4.0, 'eon': 2.0, 'xcent': 4.4, 'elantra': 11.45, 'creta': 12.9, 'verna': 5.95, 'city': 11.5, 'brio': 5.3, 'amaze': 3.75, 'jazz': 6.4}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "df = data[[\"Car_Name\", \"Selling_Price\"]]\n",
    "\n",
    "my_list = df.values.tolist()\n",
    "\n",
    "# print(my_list[0:5])\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(my_list)\n",
    "\n",
    "print(\"list elements:\", rdd.collect())\n",
    "\n",
    "rdd_df = rdd.collect()\n",
    "\n",
    "# Converting to a dictionary\n",
    "data_dict = dict(rdd_df)\n",
    "\n",
    "print(\"dictionaries:\", data_dict)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152ab3e2-eebb-4f7a-8b38-bf230a7a32e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ritz', 3.35], ['sx4', 4.75], ['ciaz', 7.25], ['wagon r', 2.85], ['swift', 4.6]]\n",
      "{'Selling_Price', 'Car_Name'}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "df = data[[\"Car_Name\", \"Selling_Price\"]]\n",
    "\n",
    "my_list = df.values.tolist()\n",
    "\n",
    "print(my_list[0:5])\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(my_list)\n",
    "\n",
    "rdd_df = rdd.collect()\n",
    "\n",
    "#Converting to a set \n",
    "rdd_set = set(df)\n",
    "\n",
    "print(rdd_set)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dabf5-31a1-402b-ab65-ee4d799f47f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color=red>Getting partitions used by resource </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf16544-d96c-43a2-9cfc-75444b05a4e9",
   "metadata": {},
   "source": [
    "- ###  <font color=red> getNumPartitions() </font>\n",
    " This a RDD function which returns a number of partitions our dataset split into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2cb8a-86ea-4f85-a6f2-e8c6be9f6d52",
   "metadata": {},
   "source": [
    "<font color=blue> you can set the partitions manually inside the parallelize() method </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9630dad4-7733-4b3a-a7fe-80f80874055b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial partition count:16\n",
      "[3.35, 4.75, 7.25, 2.85, 4.6, 9.25, 6.75, 6.5, 8.75, 7.45, 2.85, 6.85, 7.5, 6.1, 2.25, 7.75, 7.25, 7.75, 3.25, 2.65, 2.85, 4.9, 4.4, 2.5, 2.9, 3.0, 4.15, 6.0, 1.95, 7.45, 3.1, 2.35, 4.95, 6.0, 5.5, 2.95, 4.65, 0.35, 3.0, 2.25, 5.85, 2.55, 1.95, 5.5, 1.25, 7.5, 2.65, 1.05, 5.8, 7.75, 14.9, 23.0, 18.0, 16.0, 2.75, 3.6, 4.5, 4.75, 4.1, 19.99, 6.95, 4.5, 18.75, 23.5, 33.0, 4.75, 19.75, 9.25, 4.35, 14.25, 3.95, 4.5, 7.45, 2.65, 4.9, 3.95, 5.5, 1.5, 5.25, 14.5, 14.73, 4.75, 23.0, 12.5, 3.49, 2.5, 35.0, 5.9, 3.45, 4.75, 3.8, 11.25, 3.51, 23.0, 4.0, 5.85, 20.75, 17.0, 7.05, 9.65, 1.75, 1.7, 1.65, 1.45, 1.35, 1.35, 1.35, 1.25, 1.2, 1.2, 1.2, 1.15, 1.15, 1.15, 1.15, 1.11, 1.1, 1.1, 1.1, 1.05, 1.05, 1.05, 1.05, 1.0, 0.95, 0.9, 0.9, 0.75, 0.8, 0.78, 0.75, 0.75, 0.75, 0.72, 0.65, 0.65, 0.65, 0.65, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.52, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.48, 0.48, 0.48, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.42, 0.42, 0.4, 0.4, 0.4, 0.4, 0.4, 0.38, 0.38, 0.35, 0.35, 0.35, 0.31, 0.3, 0.3, 0.3, 0.27, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.18, 0.17, 0.16, 0.15, 0.12, 0.1, 3.25, 4.4, 2.95, 2.75, 5.25, 5.75, 5.15, 7.9, 4.85, 3.1, 11.75, 11.25, 2.9, 5.25, 4.5, 2.9, 3.15, 6.45, 4.5, 3.5, 4.5, 6.0, 8.25, 5.11, 2.7, 5.25, 2.55, 4.95, 3.1, 6.15, 9.25, 11.45, 3.9, 5.5, 9.1, 3.1, 11.25, 4.8, 2.0, 5.35, 4.75, 4.4, 6.25, 5.95, 5.2, 3.75, 5.95, 4.0, 5.25, 12.9, 5.0, 5.4, 7.2, 5.25, 3.0, 10.25, 8.5, 8.4, 3.9, 9.15, 5.5, 4.0, 6.6, 4.0, 6.5, 3.65, 8.35, 4.8, 6.7, 4.1, 3.0, 7.5, 2.25, 5.3, 10.9, 8.65, 9.7, 6.0, 6.25, 5.25, 2.1, 8.25, 8.99, 3.5, 7.4, 5.65, 5.75, 8.4, 10.11, 4.5, 5.4, 6.4, 3.25, 3.75, 8.55, 9.5, 4.0, 3.35, 11.5, 5.3]\n",
      "Execution time: 0.040078163146972656 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark sessiona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "df = data[\"Selling_Price\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rdd_df = spark.sparkContext.parallelize(df)\n",
    "\n",
    "print(\"initial partition count:\"+str(rdd_df.getNumPartitions()))\n",
    "\n",
    "print(rdd_df.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e0262fc-a463-4d3a-bda4-c13f80a4a2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition count is:20\n",
      "[3.35, 4.75, 7.25, 2.85, 4.6, 9.25, 6.75, 6.5, 8.75, 7.45, 2.85, 6.85, 7.5, 6.1, 2.25, 7.75, 7.25, 7.75, 3.25, 2.65, 2.85, 4.9, 4.4, 2.5, 2.9, 3.0, 4.15, 6.0, 1.95, 7.45, 3.1, 2.35, 4.95, 6.0, 5.5, 2.95, 4.65, 0.35, 3.0, 2.25, 5.85, 2.55, 1.95, 5.5, 1.25, 7.5, 2.65, 1.05, 5.8, 7.75, 14.9, 23.0, 18.0, 16.0, 2.75, 3.6, 4.5, 4.75, 4.1, 19.99, 6.95, 4.5, 18.75, 23.5, 33.0, 4.75, 19.75, 9.25, 4.35, 14.25, 3.95, 4.5, 7.45, 2.65, 4.9, 3.95, 5.5, 1.5, 5.25, 14.5, 14.73, 4.75, 23.0, 12.5, 3.49, 2.5, 35.0, 5.9, 3.45, 4.75, 3.8, 11.25, 3.51, 23.0, 4.0, 5.85, 20.75, 17.0, 7.05, 9.65, 1.75, 1.7, 1.65, 1.45, 1.35, 1.35, 1.35, 1.25, 1.2, 1.2, 1.2, 1.15, 1.15, 1.15, 1.15, 1.11, 1.1, 1.1, 1.1, 1.05, 1.05, 1.05, 1.05, 1.0, 0.95, 0.9, 0.9, 0.75, 0.8, 0.78, 0.75, 0.75, 0.75, 0.72, 0.65, 0.65, 0.65, 0.65, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.52, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.48, 0.48, 0.48, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.42, 0.42, 0.4, 0.4, 0.4, 0.4, 0.4, 0.38, 0.38, 0.35, 0.35, 0.35, 0.31, 0.3, 0.3, 0.3, 0.27, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.18, 0.17, 0.16, 0.15, 0.12, 0.1, 3.25, 4.4, 2.95, 2.75, 5.25, 5.75, 5.15, 7.9, 4.85, 3.1, 11.75, 11.25, 2.9, 5.25, 4.5, 2.9, 3.15, 6.45, 4.5, 3.5, 4.5, 6.0, 8.25, 5.11, 2.7, 5.25, 2.55, 4.95, 3.1, 6.15, 9.25, 11.45, 3.9, 5.5, 9.1, 3.1, 11.25, 4.8, 2.0, 5.35, 4.75, 4.4, 6.25, 5.95, 5.2, 3.75, 5.95, 4.0, 5.25, 12.9, 5.0, 5.4, 7.2, 5.25, 3.0, 10.25, 8.5, 8.4, 3.9, 9.15, 5.5, 4.0, 6.6, 4.0, 6.5, 3.65, 8.35, 4.8, 6.7, 4.1, 3.0, 7.5, 2.25, 5.3, 10.9, 8.65, 9.7, 6.0, 6.25, 5.25, 2.1, 8.25, 8.99, 3.5, 7.4, 5.65, 5.75, 8.4, 10.11, 4.5, 5.4, 6.4, 3.25, 3.75, 8.55, 9.5, 4.0, 3.35, 11.5, 5.3]\n",
      "Execution time: 0.039556264877319336 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "df = data[\"Selling_Price\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rdd_gnp = spark.sparkContext.parallelize(df,20)\n",
    "\n",
    "print(\"partition count is:\"+str(rdd_gnp.getNumPartitions()))\n",
    "\n",
    "print(rdd_gnp.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a3df878-9932-4062-9e94-2835f7517cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition count is:5\n",
      "[3.35, 4.75, 7.25, 2.85, 4.6, 9.25, 6.75, 6.5, 8.75, 7.45, 2.85, 6.85, 7.5, 6.1, 2.25, 7.75, 7.25, 7.75, 3.25, 2.65, 2.85, 4.9, 4.4, 2.5, 2.9, 3.0, 4.15, 6.0, 1.95, 7.45, 3.1, 2.35, 4.95, 6.0, 5.5, 2.95, 4.65, 0.35, 3.0, 2.25, 5.85, 2.55, 1.95, 5.5, 1.25, 7.5, 2.65, 1.05, 5.8, 7.75, 14.9, 23.0, 18.0, 16.0, 2.75, 3.6, 4.5, 4.75, 4.1, 19.99, 6.95, 4.5, 18.75, 23.5, 33.0, 4.75, 19.75, 9.25, 4.35, 14.25, 3.95, 4.5, 7.45, 2.65, 4.9, 3.95, 5.5, 1.5, 5.25, 14.5, 14.73, 4.75, 23.0, 12.5, 3.49, 2.5, 35.0, 5.9, 3.45, 4.75, 3.8, 11.25, 3.51, 23.0, 4.0, 5.85, 20.75, 17.0, 7.05, 9.65, 1.75, 1.7, 1.65, 1.45, 1.35, 1.35, 1.35, 1.25, 1.2, 1.2, 1.2, 1.15, 1.15, 1.15, 1.15, 1.11, 1.1, 1.1, 1.1, 1.05, 1.05, 1.05, 1.05, 1.0, 0.95, 0.9, 0.9, 0.75, 0.8, 0.78, 0.75, 0.75, 0.75, 0.72, 0.65, 0.65, 0.65, 0.65, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.52, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.48, 0.48, 0.48, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.42, 0.42, 0.4, 0.4, 0.4, 0.4, 0.4, 0.38, 0.38, 0.35, 0.35, 0.35, 0.31, 0.3, 0.3, 0.3, 0.27, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.18, 0.17, 0.16, 0.15, 0.12, 0.1, 3.25, 4.4, 2.95, 2.75, 5.25, 5.75, 5.15, 7.9, 4.85, 3.1, 11.75, 11.25, 2.9, 5.25, 4.5, 2.9, 3.15, 6.45, 4.5, 3.5, 4.5, 6.0, 8.25, 5.11, 2.7, 5.25, 2.55, 4.95, 3.1, 6.15, 9.25, 11.45, 3.9, 5.5, 9.1, 3.1, 11.25, 4.8, 2.0, 5.35, 4.75, 4.4, 6.25, 5.95, 5.2, 3.75, 5.95, 4.0, 5.25, 12.9, 5.0, 5.4, 7.2, 5.25, 3.0, 10.25, 8.5, 8.4, 3.9, 9.15, 5.5, 4.0, 6.6, 4.0, 6.5, 3.65, 8.35, 4.8, 6.7, 4.1, 3.0, 7.5, 2.25, 5.3, 10.9, 8.65, 9.7, 6.0, 6.25, 5.25, 2.1, 8.25, 8.99, 3.5, 7.4, 5.65, 5.75, 8.4, 10.11, 4.5, 5.4, 6.4, 3.25, 3.75, 8.55, 9.5, 4.0, 3.35, 11.5, 5.3]\n",
      "Execution time: 0.04695725440979004 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "df = data[\"Selling_Price\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rdd_gnp = spark.sparkContext.parallelize(df,5)\n",
    "\n",
    "print(\"partition count is:\"+str(rdd_gnp.getNumPartitions()))\n",
    "\n",
    "print(rdd_gnp.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155eaeb-af95-4715-b3e6-78e7bc5c842e",
   "metadata": {},
   "source": [
    "# <font color=red> RDD Repartition() vs Coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0577f1c-578c-4ae1-9dae-5f25159503c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local cores : 16\n",
      "parallelize : 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "# Create spark session with local cores\n",
    "rdd = spark.sparkContext.parallelize(range(0,1000))\n",
    "print(\"From local cores : \"+str(rdd.getNumPartitions()))\n",
    "\n",
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616eab4-d182-4851-9139-0b8a9322f6ab",
   "metadata": {},
   "source": [
    "- Partition 1 : 0 1 2\n",
    "- Partition 2 : 3 4 5\n",
    "- Partition 3 : 6 7 8 9\n",
    "- Partition 4 : 10 11 12\n",
    "- Partition 5 : 13 14 15\n",
    "- Partition 6 : 16 17 18 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4ea41d0-e815-4eac-af7f-458a6d8200bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions : 30\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "rddFromFile = spark.sparkContext.parallelize(data, 30)\n",
    "print(\"Partitions : \"+str(rddFromFile.getNumPartitions()))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a6ea5-798b-446a-8a79-98a4e96af26a",
   "metadata": {},
   "source": [
    "# <font color=red>RDD repartition()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc1480f9-c5b4-443f-b3a5-23cb699e1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextFile : 10\n",
      "TextFile : 6\n",
      "TextFile : 20\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "rddFromFile = spark.sparkContext.parallelize(data, 10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))\n",
    "\n",
    "rdd_rePartition = rddFromFile.repartition(6) \n",
    "print(\"TextFile : \"+str(rdd_rePartition.getNumPartitions()))\n",
    "\n",
    "rdd_rePartition1 = rddFromFile.repartition(20) \n",
    "print(\"TextFile : \"+str(rdd_rePartition1.getNumPartitions()))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ed51f-c57f-4d9a-b765-721657d93aea",
   "metadata": {},
   "source": [
    "# <font color=red>RDD coalesce()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e291dfef-9ae5-4d52-b774-0c5038af5d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial count: 16\n",
      "TextFile : 10\n",
      "TextFile : 4\n",
      "TextFile : 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "data = pd.read_csv(\"../input/car_data.csv\")\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "print(\"initial count:\", rdd.getNumPartitions())\n",
    "\n",
    "rddFromFile = spark.sparkContext.parallelize(data, 10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))\n",
    "\n",
    "rdd_coalesce = rddFromFile.coalesce(4) \n",
    "print(\"TextFile : \"+str(rdd_coalesce.getNumPartitions()))\n",
    "\n",
    "rdd_coalesce1 = rddFromFile.coalesce(11) \n",
    "print(\"TextFile : \"+str(rdd_coalesce1.getNumPartitions()))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce39f74d-bf01-459a-895c-bf37519c214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Count: 1\n",
      "After Partioning : 6\n",
      "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "|     Car_Name|Year|Selling_Price|Present_Price|Kms_Driven|Fuel_Type|Seller_Type|Transmission|Owner|\n",
      "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "|         ritz|2014|         3.35|         5.59|     27000|   Petrol|     Dealer|      Manual|    0|\n",
      "|          sx4|2013|         4.75|         9.54|     43000|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2017|         7.25|         9.85|      6900|   Petrol|     Dealer|      Manual|    0|\n",
      "|      wagon r|2011|         2.85|         4.15|      5200|   Petrol|     Dealer|      Manual|    0|\n",
      "|        swift|2014|          4.6|         6.87|     42450|   Diesel|     Dealer|      Manual|    0|\n",
      "|vitara brezza|2018|         9.25|         9.83|      2071|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|         6.75|         8.12|     18796|   Petrol|     Dealer|      Manual|    0|\n",
      "|      s cross|2015|          6.5|         8.61|     33429|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2016|         8.75|         8.89|     20273|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|         7.45|         8.92|     42367|   Diesel|     Dealer|      Manual|    0|\n",
      "|     alto 800|2017|         2.85|          3.6|      2135|   Petrol|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|         6.85|        10.38|     51000|   Diesel|     Dealer|      Manual|    0|\n",
      "|         ciaz|2015|          7.5|         9.94|     15000|   Petrol|     Dealer|   Automatic|    0|\n",
      "|       ertiga|2015|          6.1|         7.71|     26000|   Petrol|     Dealer|      Manual|    0|\n",
      "|        dzire|2009|         2.25|         7.21|     77427|   Petrol|     Dealer|      Manual|    0|\n",
      "|       ertiga|2016|         7.75|        10.79|     43000|   Diesel|     Dealer|      Manual|    0|\n",
      "|       ertiga|2015|         7.25|        10.79|     41678|   Diesel|     Dealer|      Manual|    0|\n",
      "|       ertiga|2016|         7.75|        10.79|     43000|   Diesel|     Dealer|      Manual|    0|\n",
      "|      wagon r|2015|         3.25|         5.09|     35500|      CNG|     Dealer|      Manual|    0|\n",
      "|          sx4|2010|         2.65|         7.98|     41442|   Petrol|     Dealer|      Manual|    0|\n",
      "+-------------+----+-------------+-------------+----------+---------+-----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "data = spark.read.csv(\"../input/car_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Initial Count:\", data.rdd.getNumPartitions())\n",
    "\n",
    "rdd = data.rdd.repartition(6) \n",
    "\n",
    "print(\"After Partioning : \"+str(rdd.getNumPartitions()))\n",
    "\n",
    "data.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cc988-1ba6-40c8-a263-3c274f629d14",
   "metadata": {},
   "source": [
    "# <font color=red> RDD Operations </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf66c4e-fb67-4c9a-9f4d-85dd337c81f8",
   "metadata": {},
   "source": [
    "- RDD has two internal operations\n",
    "\n",
    "RDD transformations – Transformations are lazy operations, instead of updating an RDD, these operations return another RDD.\n",
    "RDD actions – operations that trigger computation and return RDD values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d6c92-1b54-4e86-9415-3635796273d4",
   "metadata": {},
   "source": [
    "Transformations on PySpark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return new RDD instead of updating the current."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20650b0-3ec1-4da8-b87e-b083997195a6",
   "metadata": {},
   "source": [
    "<font color=red> flatMap </font> \n",
    "- flatMap() transformation flattens the RDD after applying the function and returns a new RDD. On the below example, first, it splits each record by space in an RDD and finally flattens it. Resulting RDD consists of a single word on each record.\n",
    "- In Apache Spark's Resilient Distributed Dataset (RDD) API, the flatMap() transformation is used to apply a function to each element of the RDD and then flatten the resulting sequence of sequences into a single RDD. This is particularly useful when you want to transform each element into multiple elements and then merge those elements into a single RDD, essentially \"flattening\" the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ca0178-a353-4694-917d-18bfdfc8d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "    \n",
    "rdd = spark.sparkContext.textFile(\"../input/test.txt\")\n",
    "\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ac7a6-0b5b-4ac1-bd70-ca9505d79cf3",
   "metadata": {},
   "source": [
    "<font color=red> map </font>\n",
    "- map() transformation is used to apply any complex operations like adding a column, updating a column e.t.c, the output of map transformations would always have the same number of records as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027c744-477f-45de-9176-c9af4ac9d501",
   "metadata": {},
   "source": [
    "- Key points\n",
    "\n",
    "- Both map() & flatMap() returns Dataset (DataFrame=Dataset[Row]).\n",
    "- Both these transformations are narrow meaning they do not result in Spark Data Shuffle.\n",
    "- flatMap() results in redundant data on some columns.\n",
    "- One of the use cases of flatMap() is to flatten column which contains arrays, list, or any nested collection(one cell with one value).\n",
    "- map() always return the same size/records as in input DataFrame whereas flatMap() returns many records for each record (one-many)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d44dd818-5ab9-4893-b05c-b28c90ede01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "    \n",
    "rdd = spark.sparkContext.textFile(\"../input/test.txt\")\n",
    "\n",
    "rdd3 = rdd.map(lambda x: (x,1))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcde31-a64c-4d6a-bacf-86067a033732",
   "metadata": {},
   "source": [
    "<font color=red> reduceByKey </font>\n",
    "- reduceByKey() merges the values for each key with the function specified. In our example, it reduces the word string by applying the sum function on value. The result of our RDD contains unique words and their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebf27114-be13-4e1a-a233-403ef11f2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "    \n",
    "rdd = spark.sparkContext.textFile(\"../input/test.txt\")\n",
    "\n",
    "rdd4 = rdd.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c809a-275b-4cc5-91ec-f45fdfd20bb2",
   "metadata": {},
   "source": [
    "<font color=red> sortByKey </font> \n",
    "- sortByKey() transformation is used to sort RDD elements on key. In our example, first, we convert RDD[(String,Int]) to RDD[(Int, String]) using map transformation and apply sortByKey which ideally does sort on an integer value. And finally, foreach with println statements returns all words in RDD and their count as key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4352a5bd-fcac-4105-9dc0-9903591f490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"../input/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542821e-03c0-4071-b749-133ec0ebaccb",
   "metadata": {},
   "source": [
    "<font color=red> filter </font> \n",
    "- filter() transformation is used to filter the records in an RDD. In our example we are filtering all words starts with “a”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c306508-2634-48e7-9353-ca96626bf344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339cdc6d-a864-48ee-ad05-6cac7c59c222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935ad1a-041b-4662-8fc1-e3c39f349d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae88f3-958b-4440-8eb6-cae82762d033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
